# -*- coding: utf-8 -*-
"""cifar10_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VnIHSkEg0at1RGo-9AvVybSJW3o_FUF4
"""

import numpy
from keras.models import Sequential
from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D

from keras.datasets import cifar10
# let's load data 
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

#normalizing inputs from 0-255 to 0.0-1.0 
X_train = X_train.astype('float32') 
X_test = X_test.astype('float32') 
X_train = X_train / 255.0 
X_test = X_test / 255.0

X_train.shape, X_test.shape, y_train.shape, y_test.shape

# one hot encode outputs 
from keras.utils import np_utils 

y_train = np_utils.to_categorical(y_train) 
y_test = np_utils.to_categorical(y_test) 
num_classes = y_test.shape[1]

import keras
from keras.constraints import maxnorm
from keras.optimizers import SGD
from keras import backend as K

model = Sequential()

# 2D concolutional layer.32 == filters needed.(an array of numeric values). (3,3)==size of the filter(3 r, 3 c)
# 32,32,3 - shape of data-h,w,rgb values(depth/intensity)
# relu activation replaces -ve pixel values with 0s.
model.add(Conv2D(32, (3, 3), input_shape=(32,32,3), activation='relu', padding='same'))  #output=feature maps
# dropout used to prevent overfitting
model.add(Dropout(0.2)) 
model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) 
model.add(MaxPooling2D(pool_size=(2, 2))) 
# maxpooling==dim reduction - while retaining most important features
model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) 
model.add(Dropout(0.2)) 
model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) 
model.add(MaxPooling2D(pool_size=(2, 2))) 
model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) 
model.add(Dropout(0.2)) 
model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) 
model.add(MaxPooling2D(pool_size=(2, 2))) 
# flatten used to convert feature map to 1D.
model.add(Flatten())
model.add(Dropout(0.2)) 
model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3))) 
model.add(Dropout(0.2)) 
model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3))) 
model.add(Dropout(0.2)) 
# softmax used - multiclass classification
model.add(Dense(num_classes, activation='softmax'))

model.summary()

# model compilation
learning_rate = 0.01
epochs = 50 # we want a high quality model 
decay = learning_rate/epochs 
sgd = SGD(lr=learning_rate, momentum=0.9, decay=decay, nesterov=False) #optimisation function - stochastic gradient descent
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# model training
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)

# Final evaluation of the model 
scores = model.evaluate(X_test, y_test, verbose=0) 
print("Accuracy: %.2f%%" % (scores[1]*100))