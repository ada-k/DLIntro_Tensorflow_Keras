{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_model_performance.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BEGzuSEGgpwQ",
        "2IjL8Fbbjma-",
        "8z-6ND4qo8zw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1BrtPCPjgZo"
      },
      "source": [
        "## Improvig Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEGzuSEGgpwQ"
      },
      "source": [
        "### Learning Curves\n",
        "\n",
        "> Building a model on the digits dataset, a sample dataset that comes pre-loaded with scikit learn. The digits dataset consist of 8x8 pixel handwritten digits from 0 to 9:\n",
        "\n",
        "> We want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification.\n",
        "\n",
        "> The dataset has already been partitioned into X_train, y_train, X_test, and y_test, using 30% of the data as testing data. The labels are already one-hot encoded vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FhsVC26gV87"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Instantiate a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
        "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
        "\n",
        "# Output layer with 10 neurons (one per digit) and softmax\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "# Compile your model\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Test if your model is well assembled by predicting before training\n",
        "print(model.predict(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYhEGenyhpfm"
      },
      "source": [
        "def plot_loss(loss,val_loss):\n",
        "    plt.figure()\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
        "h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
        "\n",
        "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
        "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvrrYIuojOzB"
      },
      "source": [
        "* alot of predefined variables and functions for the next cells.\n",
        "* some implementations are in the 'Untitled' kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB4M_I0XiL0G"
      },
      "source": [
        "for size in training_sizes:\n",
        "  \t# Get a fraction of training data (we only care about the training data)\n",
        "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
        "\n",
        "    # Reset the model to the initial weights and train it on the new training data fraction\n",
        "    model.set_weights(initial_weights)\n",
        "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
        "\n",
        "    # Evaluate and store both: the training data fraction and the complete test set results\n",
        "    train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1])\n",
        "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
        "    \n",
        "# Plot train vs test accuracies\n",
        "plot_results(train_accs, test_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqar3D9qjZYA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IjL8Fbbjma-"
      },
      "source": [
        "### Activation Functions\n",
        "\n",
        "* **sigmoid**: 0-1\n",
        "avoid sigmoi and tune with experimentation. \n",
        "* **tanh**: -1 - 1\n",
        "* **relu** (rectified linear unit): 0 -infinity.\n",
        "train fast and generalise most problems: always start with this.\n",
        "\n",
        "* **leaky relu**: smoothened relu. (allows negative values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCMdeq0KjpRo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAMcDvUzmh0U"
      },
      "source": [
        "# Activation functions to try\n",
        "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
        "\n",
        "# Loop over the activation functions\n",
        "activation_results = {}\n",
        "\n",
        "for act in activations:\n",
        "  # Get a new model with the current activation\n",
        "  model = get_model(act)\n",
        "  # Fit the model and store the history results\n",
        "  h_callback = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 20, verbose = 0)\n",
        "  activation_results[act] = h_callback\n",
        "\n",
        "\n",
        "# val_loss_per_function = h_callback.history['val_loss']\n",
        "# val_acc_per_function = h_callback.history['val_acc']\n",
        "\n",
        "# Create a dataframe from val_loss_per_function\n",
        "val_loss= pd.DataFrame(val_loss_per_function)\n",
        "\n",
        "# Call plot on the dataframe\n",
        "val_loss.plot()\n",
        "plt.show()\n",
        "\n",
        "# Create a dataframe from val_acc_per_function\n",
        "val_acc = pd.DataFrame(val_acc_per_function)\n",
        "\n",
        "# Call plot on the dataframe\n",
        "val_acc.plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z-6ND4qo8zw"
      },
      "source": [
        "### Batch Size and Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhjPWQlto_n0"
      },
      "source": [
        "# Get a fresh new model with get_model\n",
        "model = get_model()\n",
        "\n",
        "# Train your model for 5 epochs with a batch size of 1\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
        "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
        "      model.evaluate(X_test, y_test)[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "643BZqh_qgTA"
      },
      "source": [
        "model = get_model()\n",
        "\n",
        "# Fit your model for 5 epochs with a batch of size the training set\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=X_train.shape[0])\n",
        "print(\"\\n The accuracy when using the whole training set as batch-size was: \",\n",
        "      model.evaluate(X_test, y_test)[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFTyJN-Kq37J"
      },
      "source": [
        "You can see that accuracy is lower when using a batch size equal to the training set size. This is not because the network had more trouble learning the optimization function: Even though the same number of epochs were used for both batch sizes the number of resulting weight updates was very different!. With a batch of size the training set and 5 epochs we only get 5 updates total, each update computes and averaged gradient descent with all the training set observations. To obtain similar results with this batch size we should increase the number of epochs so that more weight updates take place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8tqG_CNrfn4"
      },
      "source": [
        "Building a new deeper model consisting of 3 hidden layers of 50 neurons each, using batch normalization in between layers. The kernel_initializer parameter is used to initialize weights in a similar way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIbaiIKBrMo7"
      },
      "source": [
        "# Import batch normalization from keras layers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "# Build your deep network\n",
        "batchnorm_model = Sequential()\n",
        "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
        "batchnorm_model.add(BatchNormalization())\n",
        "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
        "batchnorm_model.add(BatchNormalization())\n",
        "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
        "batchnorm_model.add(BatchNormalization())\n",
        "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "# Compile your model with sgd\n",
        "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HRUURPgsD0W"
      },
      "source": [
        "def compare_histories_acc(h1,h2):\n",
        "    plt.plot(h1.history['acc'])\n",
        "    plt.plot(h1.history['val_acc'])\n",
        "    plt.plot(h2.history['acc'])\n",
        "    plt.plot(h2.history['val_acc'])\n",
        "    plt.title(\"Batch Normalization Effects\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Test', 'Train with Batch Normalization', 'Test with Batch Normalization'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "# Train your standard model, storing its history callback\n",
        "h1_callback = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
        "\n",
        "# Train the batch normalized model you recently built, store its history callback\n",
        "h2_callback = batchnorm_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
        "\n",
        "# Call compare_histories_acc passing in both model histories\n",
        "compare_histories_acc(h1_callback, h2_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxJ3HMPgswCE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlCafiXdsw7l"
      },
      "source": [
        "### Hyperparameters Tuning\n",
        "\n",
        "* NUmber of layers\n",
        "* neurons per layer\n",
        "* order of layers\n",
        "* batch size\n",
        "* activation function\n",
        "* optimisers\n",
        "* learning rate\n",
        "\n",
        "> to utilise scikitlearn methods like randomsearchcv, we first conver our keras model to a scikitlearn estimator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4tM3UPuu81X"
      },
      "source": [
        "the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.\n",
        "\n",
        "Build a simple create_model() function that receives both a learning rate and an activation function as arguments. The Adam optimizer has been imported as an object from keras.optimizers so that you can also change its learning rate parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVZszg9Ms0ei"
      },
      "source": [
        "# Creates a model given an activation and learning rate\n",
        "def create_model(learning_rate, activation):\n",
        "  \n",
        "  \t# Create an Adam optimizer with the given learning rate\n",
        "  \topt = Adam(lr = learning_rate)\n",
        "  \t\n",
        "  \t# Create your binary classification model  \n",
        "  \tmodel = Sequential()\n",
        "  \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
        "  \tmodel.add(Dense(256, activation = activation))\n",
        "  \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
        "  \t\n",
        "  \t# Compile your model with your optimizer, loss, and metrics\n",
        "  \tmodel.compile(optimizer = opt, loss = 'binary cross-entropy', metrics = ['accuracy'])\n",
        "  \treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V19bekAXu2--"
      },
      "source": [
        "# Import KerasClassifier from keras scikit learn wrappers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Create a KerasClassifier\n",
        "model = KerasClassifier(build_fn = create_model)\n",
        "\n",
        "# Define the parameters to try out\n",
        "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
        "          'epochs': [50, 100, 200], 'learning_rate': [0.1, .01, .001]}\n",
        "\n",
        "# Create a randomize search cv object passing in the parameters to try\n",
        "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
        "\n",
        "\n",
        "#fit and print out the best parameters\n",
        "random_search.fit(X,y)\n",
        "\n",
        "random_search.best_params_\n",
        "# # Running random_search.fit(X,y) would start the search,but it takes too long! \n",
        "# show_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK_ByeUrwY0L"
      },
      "source": [
        "# Import KerasClassifier from keras wrappers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Create a KerasClassifier\n",
        "model = KerasClassifier(build_fn = create_model(learning_rate = .001, activation = 'relu'), epochs = 50, batch_size = 128, verbose = 0)\n",
        "\n",
        "# Calculate the accuracy score for each fold\n",
        "kfolds = cross_val_score(model, X, y, cv = 3)\n",
        "\n",
        "# Print the mean accuracy\n",
        "print('The mean accuracy was:', kfolds.mean())\n",
        "\n",
        "# Print the accuracy standard deviation\n",
        "print('With a standard deviation of:', kfolds.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKsweACxCQX"
      },
      "source": [
        ""
      ]
    }
  ]
}